{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import  StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, GBTRegressor\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# from pyspark.ml.torch.distributor import TorchDistributor\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "# from utils import read_process_df, prepare_data, evaluate_model, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting default log level to \"WARN\".\n",
    "# To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/09 22:23:34 WARN Utils: Your hostname, Bhaiyu resolves to a loopback address: 127.0.1.1; using 172.17.120.207 instead (on interface eth0)\n",
      "23/09/09 22:23:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/abhishek-wsl/miniconda3/envs/mlops/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/abhishek-wsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/abhishek-wsl/.ivy2/jars\n",
      "org.mlflow#mlflow-spark added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e42da452-aa6b-4da8-9732-153723f9a5f6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mlflow#mlflow-spark;2.2.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      ":: resolution report :: resolve 178ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\torg.mlflow#mlflow-spark;2.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e42da452-aa6b-4da8-9732-153723f9a5f6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n",
      "23/09/09 22:23:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.120.207:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>modelling</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7feb05f726a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('modelling').config(\"spark.driver.memory\", \"2g\").config(\"spark.executor.memory\", \"6g\")\\\n",
    "    .config('spark.executor.cores',2).config('spark.default.parallelism',4).config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:2.2.0\")\\\n",
    "       .getOrCreate() # .config(\"spark.sql.shuffle.partitions\",10)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.spark.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/abhishek-wsl/codes/MLops_project/final_codes/mlruns/1', creation_time=1693228766019, experiment_id='1', last_update_time=1693228766019, lifecycle_stage='active', name='modelling_duration_exp', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(experiment_name=\"modelling_duration_exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.appName('modelling').config(\"spark.driver.memory\", \"5g\").config(\"spark.rapids.force.caller.classloader\",\"false\" )\\\n",
    "#     .config(\"spark.executor.extraClassPath\", r\"\\home\\abhishek-wsl\\rapid_plugin\\rapids-4-spark_2.12-23.08.1.jar\")\\\n",
    "#     .config(\"spark.plugins\",\"com.nvidia.spark.SQLPlugin\")\\\n",
    "#         .config(\"spark.rapids.sql.enabled\",\"true\").config(\"spark.driver.resource.gpu.discoveryScript\",\"./discover_gpu.sh\")\\\n",
    "#     .config(\"spark.executor.resource.gpu.amount\", \"2\")\\\n",
    "#     .config(\"spark.driver.resource.gpu.amount\", \"1\").config(\"spark.sql.shuffle.partitions\",10).getOrCreate()\n",
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2g'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.driver.memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()\n",
    "#df.show(5)\n",
    "#df.select('VendorID').distinct().show()\n",
    "#df.select('duration').summary(\"count\", \"min\", \"1%\",\"25%\", \"50%\", \"75%\", \"95%\", \"98%\",  \"99%\",\"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- minimum duration is -54\n",
    "- max looks like heavily skewed\n",
    "- lets consider minimum as 0.05 min (1 percentile) and max as 82 mins (99 percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_process_df(path):\n",
    "    df = spark.read.format('parquet').load(path)\n",
    "    df = df.select('VendorID','lpep_pickup_datetime','lpep_dropoff_datetime','PULocationID','DOLocationID','trip_distance')\n",
    "    df = df.withColumn('duration',\\\n",
    "        round((col('lpep_dropoff_datetime')-col('lpep_pickup_datetime'))\\\n",
    "        .cast(\"long\")/60,2))\n",
    "    df = df.filter(col('duration')>=0.05).filter(col('duration')<=82)\n",
    "    df = df.withColumn('PU_DO',concat(col('PULocationID'),lit('_'),col('DOLocationID')))\n",
    "    df = df.withColumn('pu_hour',hour(col('lpep_pickup_datetime')))\n",
    "    df = df.withColumn('pu_weekday',dayofweek(col('lpep_pickup_datetime')))\n",
    "    \n",
    "    df = df.select('VendorID','pu_hour','pu_weekday','PU_DO', 'trip_distance','duration')\n",
    "    # y = df.select('')\n",
    "\n",
    "    print(df.count(), len(df.columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = sdf.toPandas() #.describe()\n",
    "# sdf = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_processed,categorical_cols,indexer_final=None,encoder_final=None, is_test=True):\n",
    "    indexers = [StringIndexer(inputCol=col,outputCol=col+'_index').fit(df_processed) for col in categorical_cols]\n",
    "    if is_test:\n",
    "        df_processed = df_processed.dropna(subset='duration')\n",
    "        [indexer.setHandleInvalid(\"keep\") for indexer in indexers]\n",
    "    indexer_pipeline = Pipeline(stages=indexers)\n",
    "    if indexer_final==None:\n",
    "        indexer_final = indexer_pipeline.fit(df_processed)\n",
    "    \n",
    "    indexed_df = indexer_final.transform(df_processed)\n",
    "\n",
    "    encoder = [OneHotEncoder(inputCol=col+'_index',outputCol=col+'_onehot') for col in categorical_cols]\n",
    "    encoder_pipeline = Pipeline(stages = encoder)\n",
    "    if encoder_final==None:\n",
    "        encoder_final = encoder_pipeline.fit(indexed_df)\n",
    "    encoded_df = encoder_final.transform(indexed_df)\n",
    "    \n",
    "    return encoded_df, indexer_final, encoder_final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoded_df,feature_cols,label_col,regressor = LinearRegression,**kwargs):\n",
    "    assembler = VectorAssembler(inputCols = feature_cols, outputCol = 'features')\n",
    "    regressor = regressor(featuresCol = 'features', labelCol= label_col,**kwargs )\n",
    "    pipeline = Pipeline(stages = [assembler,regressor])\n",
    "    model = pipeline.fit(encoded_df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,encoded_df,label_col,metric='rmse'):\n",
    "    predictions = model.transform(encoded_df)\n",
    "    evaluator = RegressionEvaluator(labelCol=label_col,predictionCol='prediction',metricName=metric)\n",
    "    out = evaluator.evaluate(predictions)\n",
    "    print(f\"{metric}  : {out}\")\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333262 6\n",
      "61753 6\n",
      "67613 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train_processed = read_process_df('/home/abhishek-wsl/codes/MLops_project/data/*.parquet')\n",
    "df_test_processed1 = read_process_df('/home/abhishek-wsl/codes/MLops_project/data/test_data/green_tripdata_2022-01.parquet')\n",
    "df_test_processed2 = read_process_df('/home/abhishek-wsl/codes/MLops_project/data/test_data/green_tripdata_2023-01.parquet')\n",
    "\n",
    "categorical_cols = ['VendorID','pu_hour','pu_weekday','PU_DO']\n",
    "encoded_df_train, indexer_final, encoder_final = prepare_data(df_train_processed,categorical_cols)\n",
    "encoded_df_test1, _, _ = prepare_data(df_test_processed1,categorical_cols,indexer_final,encoder_final,is_test=True)\n",
    "encoded_df_test2, _, _ = prepare_data(df_test_processed2,categorical_cols,indexer_final,encoder_final,is_test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['trip_distance','VendorID_onehot','pu_hour_onehot','pu_weekday_onehot','PU_DO_onehot']\n",
    "label_col = 'duration'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols = ['trip_distance','VendorID_onehot','pu_hour_onehot','pu_weekday_onehot','PU_DO_onehot']\n",
    "# label_col = 'duration'\n",
    "# def objective(params):\n",
    "\n",
    "#     with mlflow.start_run(run_name=\"linear regression hyperopt\"):\n",
    "#         mlflow.set_tag(\"model\",\"linear regression\")\n",
    "\n",
    "#         mlflow.log_params(params)\n",
    "#         reg = train_model(encoded_df_train,feature_cols,label_col,regressor = LinearRegression,maxIter=100,**params)\n",
    "\n",
    "#         rmse = evaluate_model(reg,encoded_df_test1,label_col,metric='rmse')\n",
    "#         mlflow.log_metric(\"val_rmse\",rmse)\n",
    "#     return {'loss':rmse, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'regParam': hp.loguniform('regParam',-2,-1),\n",
    "#     'fitIntercept': hp.choice('fitIntercept',[True,False]),\n",
    "#     'elasticNetParam': hp.uniform('elasticNetParam',0,1),\n",
    "#     # 'objective': 'reg:linear',\n",
    "#     # 'seed':7\n",
    "# }\n",
    "# best_result = fmin(\n",
    "#     fn=objective,\n",
    "#     space=search_space,\n",
    "#     algo= tpe.suggest,\n",
    "#     max_evals=50,\n",
    "#     trials=Trials()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols = ['trip_distance','VendorID_onehot','pu_hour_onehot','pu_weekday_onehot','PU_DO_onehot']\n",
    "# label_col = 'duration'\n",
    "# def objective(params):\n",
    "\n",
    "#     with mlflow.start_run(run_name=\"random forest hyperopt\"):\n",
    "#         mlflow.set_tag(\"model\",\"random forest\")\n",
    "\n",
    "#         mlflow.log_params(params)\n",
    "#         reg = train_model(encoded_df_train,feature_cols,label_col,regressor = RandomForestRegressor,**params)\n",
    "\n",
    "#         rmse = evaluate_model(reg,encoded_df_test1,label_col,metric='rmse')\n",
    "#         mlflow.log_metric(\"val_rmse\",rmse)\n",
    "#     return {'loss':rmse, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols = ['trip_distance','VendorID_onehot','pu_hour_onehot','pu_weekday_onehot','PU_DO_onehot']\n",
    "# label_col = 'duration'\n",
    "# def objective(params):\n",
    "\n",
    "#     with mlflow.start_run(run_name=\"random forest hyperopt\"):\n",
    "#         mlflow.spark.autolog()\n",
    "#         reg = train_model(encoded_df_train,feature_cols,label_col,regressor = RandomForestRegressor,**params)\n",
    "\n",
    "#         rmse = evaluate_model(reg,encoded_df_test1,label_col,metric='rmse')\n",
    "#         #mlflow.log_metric(\"val_rmse\",rmse)\n",
    "#     return {'loss':rmse, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'maxDepth': scope.int(hp.quniform('maxDepth', 4, 40, 1)),\n",
    "#     'numTrees': scope.int(hp.quniform(\"numTrees\", 5, 60, 5)),\n",
    "#     'minInfoGain': hp.uniform('minInfoGain', 0.4, 1),\n",
    "#     'minInstancesPerNode': scope.int(hp.quniform(\"minInstancesPerNode\", 500, 10000, 10)),\n",
    "#     # 'objective': 'reg:linear',\n",
    "#     'seed': 7\n",
    "# }\n",
    "# best_result = fmin(\n",
    "#     fn=objective,\n",
    "#     space=search_space,\n",
    "#     algo= tpe.suggest,\n",
    "#     max_evals=50,\n",
    "#     trials=Trials()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols = ['trip_distance','VendorID_onehot','pu_hour_onehot','pu_weekday_onehot','PU_DO_onehot']\n",
    "# label_col = 'duration'\n",
    "# with mlflow.start_run(run_name=\"gbt_model_default_params\"):\n",
    "#     mlflow.set_tag(\"developer\",\"Abhishek Singh\")\n",
    "\n",
    "#     mlflow.log_param(\"training_data_path\",\"/home/abhishek-wsl/codes/MLops_project/data/*.parquet\")\n",
    "#     mlflow.log_param(\"val_data_path\",\"/home/abhishek-wsl/codes/MLops_project/data/test_data/green_tripdata_2022-01.parquet\")\n",
    "\n",
    "#     #reg_name = GBTRegressor\n",
    "#     fitIntercept = True\n",
    "#     standardization = True\n",
    "#     # mlflow.log_params({'fitIntercept':fitIntercept,'standardization':standardization})\n",
    "#     lr_model = train_model(encoded_df_train,feature_cols,label_col,fitIntercept=fitIntercept,standardization=standardization)\n",
    "#     # rf_model = train_model(encoded_df_train,feature_cols,label_col,regressor=RandomForestRegressor)\n",
    "#     #gbt_model = train_model(encoded_df_train,feature_cols,label_col,regressor=GBTRegressor)\n",
    "#     # mlflow.reg_name.log_model(lr_model,'model')\n",
    "\n",
    "#     rmse = evaluate_model(lr_model,encoded_df_test1,label_col,metric='rmse')\n",
    "\n",
    "#     mlflow.log_metric(\"val_rmse\",rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: autolog() in mlflow-spark currently does not support Spark ml models: mlflow.spark.autolog()\n",
    "\n",
    "- The proper way would be to train and experiment models using sklearn/tensorflow \n",
    "    - and log model as mlflow model\n",
    "        - Then deploy using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'elasticNetParam':\t0.07666740311526124,\n",
    "#             'fitIntercept':\tTrue,\n",
    "#             'regParam':\t0.1452221778539822}\n",
    "\n",
    "\n",
    "\n",
    "# with mlflow.start_run(run_name=\"gbt regression\"):\n",
    "#     mlflow.set_tag(\"developer\",\"Abhishek Singh\")\n",
    "#     mlflow.set_tag(\"model\",\"GBT\")\n",
    "#     # mlflow.log_params(params)\n",
    "\n",
    "#     reg = train_model(encoded_df_train,feature_cols,label_col,regressor = GBTRegressor)\n",
    "#     rmse = evaluate_model(reg,encoded_df_test1,label_col,metric='rmse')\n",
    "#     mlflow.log_metric(\"val_rmse\",rmse)\n",
    "\n",
    "#     # reg.write().overwrite().save( \"/home/abhishek-wsl/codes/MLops_project/trained_models/lr_PipelineModel\")\n",
    "#     # mlflow.log_artifact(local_path=\"/home/abhishek-wsl/codes/MLops_project/trained_models/lr_PipelineModel\",artifact_path='lr1_spark/')\n",
    "#     mlflow.log_artifact(\"/home/abhishek-wsl/codes/MLops_project/trained_models/stringindexer_PipelineModel\", artifact_path='preprocessors')\n",
    "#     mlflow.log_artifact(\"/home/abhishek-wsl/codes/MLops_project/trained_models/encoderindexer_PipelineModel\", artifact_path='preprocessors')\n",
    "\n",
    "#     mlflow.spark.log_model(reg,artifact_path='gbt_spark_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "params = {'elasticNetParam':\t0.07666740311526124,\n",
    "            'fitIntercept':\tTrue,\n",
    "            'regParam':\t0.1452221778539822}\n",
    "\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"linear regression final\"):\n",
    "    mlflow.set_tag(\"developer\",\"Abhishek Singh\")\n",
    "    mlflow.set_tag(\"model\",\"linear regression\")\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    reg = train_model(encoded_df_train,feature_cols,label_col,regressor = LinearRegression,maxIter=100,**params)\n",
    "    rmse = evaluate_model(reg,encoded_df_test1,label_col,metric='rmse')\n",
    "    mlflow.log_metric(\"val_rmse\",rmse)\n",
    "\n",
    "    # reg.write().overwrite().save( \"/home/abhishek-wsl/codes/MLops_project/trained_models/lr_PipelineModel\")\n",
    "    # mlflow.log_artifact(local_path=\"/home/abhishek-wsl/codes/MLops_project/trained_models/lr_PipelineModel\",artifact_path='lr1_spark/')\n",
    "    # mlflow.log_artifact(\"/home/abhishek-wsl/codes/MLops_project/trained_models/stringindexer_PipelineModel\", artifact_path='preprocessors')\n",
    "    # mlflow.log_artifact(\"/home/abhishek-wsl/codes/MLops_project/trained_models/encoderindexer_PipelineModel\", artifact_path='preprocessors')\n",
    "\n",
    "    mlflow.spark.log_model(reg,artifact_path='lr_spark_final')\n",
    "    mlflow.spark.log_model(indexer_final,artifact_path='indexer')\n",
    "    mlflow.spark.log_model(encoder_final,artifact_path='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.spark.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/abhishek-wsl/miniconda3/envs/mlops/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/abhishek-wsl/miniconda3/envs/mlops/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/abhishek-wsl/miniconda3/envs/mlops/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/02 21:35:08 INFO mlflow.spark: 'runs:/eff1613c78114ce39c6b0b622b9d294e/lr_spark_final' resolved as '/home/abhishek-wsl/codes/MLops_project/final_codes/mlruns/1/eff1613c78114ce39c6b0b622b9d294e/artifacts/lr_spark_final'\n",
      "2023/09/02 21:35:08 INFO mlflow.spark: URI 'runs:/eff1613c78114ce39c6b0b622b9d294e/lr_spark_final/sparkml' does not point to the current DFS.\n",
      "2023/09/02 21:35:08 INFO mlflow.spark: File 'runs:/eff1613c78114ce39c6b0b622b9d294e/lr_spark_final/sparkml' not found on DFS. Will attempt to upload the file.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "logged_model = 'runs:/eff1613c78114ce39c6b0b622b9d294e/lr_spark_final'\n",
    "\n",
    "# Load model\n",
    "loaded_model = mlflow.spark.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_e9fe1b0caf0a"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+-------+-------------+--------+--------------+-------------+----------------+-----------+---------------+---------------+-----------------+-------------------+--------------------+------------------+\n",
      "|VendorID|pu_hour|pu_weekday|  PU_DO|trip_distance|duration|VendorID_index|pu_hour_index|pu_weekday_index|PU_DO_index|VendorID_onehot| pu_hour_onehot|pu_weekday_onehot|       PU_DO_onehot|            features|        prediction|\n",
      "+--------+-------+----------+-------+-------------+--------+--------------+-------------+----------------+-----------+---------------+---------------+-----------------+-------------------+--------------------+------------------+\n",
      "|       2|      0|         7|  42_42|         0.44|     1.2|           0.0|         17.0|             5.0|       25.0|  (3,[0],[1.0])|(24,[17],[1.0])|    (7,[5],[1.0])| (14288,[25],[1.0])|(14323,[0,1,21,33...|1.8948750528738465|\n",
      "|       1|      0|         7| 116_41|          2.1|    8.72|           1.0|         17.0|             5.0|      205.0|  (3,[1],[1.0])|(24,[17],[1.0])|    (7,[5],[1.0])|(14288,[205],[1.0])|(14323,[0,2,21,33...|   8.3719984018067|\n",
      "|       1|      0|         7| 41_140|          3.7|    16.2|           1.0|         17.0|             5.0|      211.0|  (3,[1],[1.0])|(24,[17],[1.0])|    (7,[5],[1.0])|(14288,[211],[1.0])|(14323,[0,2,21,33...|17.554534781366225|\n",
      "|       2|      0|         7|181_181|         1.69|    8.25|           0.0|         17.0|             5.0|      253.0|  (3,[0],[1.0])|(24,[17],[1.0])|    (7,[5],[1.0])|(14288,[253],[1.0])|(14323,[0,1,21,33...| 4.404324665456787|\n",
      "|       2|      0|         7| 33_170|         6.26|   21.03|           0.0|         17.0|             5.0|      713.0|  (3,[0],[1.0])|(24,[17],[1.0])|    (7,[5],[1.0])|(14288,[713],[1.0])|(14323,[0,1,21,33...|21.879785042398975|\n",
      "+--------+-------+----------+-------+-------------+--------+--------------+-------------+----------------+-----------+---------------+---------------+-----------------+-------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform inference via model.transform()\n",
    "loaded_model.transform(encoded_df_test1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse  : 6.238286150945589\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluate_model(loaded_model,encoded_df_test2,label_col,metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models=False\n",
    "if save_models:\n",
    "    lr_model.save( \"/home/abhishek-wsl/codes/MLops_project/trained_models/lr_PipelineModel\")\n",
    "    #rf_model.save(\"/home/abhishek-wsl/codes/MLops_project/trained_models/rf_PipelineModel\")\n",
    "    indexer_final.save(\"/home/abhishek-wsl/codes/MLops_project/trained_models/stringindexer_PipelineModel\")\n",
    "    encoder_final.save(\"/home/abhishek-wsl/codes/MLops_project/trained_models/encoderindexer_PipelineModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training rmse using linear regresion model:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lr_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtraining rmse using linear regresion model:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m _ \u001b[39m=\u001b[39m evaluate_model(lr_model,encoded_df_train,label_col,metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# print('training rmse using random forest regresion model:')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# _ = evaluate_model(rf_model,encoded_df_train,label_col,metric='rmse')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_model' is not defined"
     ]
    }
   ],
   "source": [
    "print('training rmse using linear regresion model:')\n",
    "_ = evaluate_model(lr_model,encoded_df_train,label_col,metric='rmse')\n",
    "# print('training rmse using random forest regresion model:')\n",
    "# _ = evaluate_model(rf_model,encoded_df_train,label_col,metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rmse Jan 2022 using linear regresion model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/27 00:03:06 WARN DAGScheduler: Broadcasting large task binary with size 1646.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse  : 7.664661368974521\n"
     ]
    }
   ],
   "source": [
    "print('test rmse Jan 2022 using linear regresion model:')\n",
    "_ = evaluate_model(lr_model,encoded_df_test1,label_col,metric='rmse')\n",
    "# print('test rmse Jan 2022 using random forest regresion model:')\n",
    "# _ = evaluate_model(rf_model,encoded_df_test1,label_col,metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rmse Jan 2023 using linear regresion model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/27 00:03:22 WARN DAGScheduler: Broadcasting large task binary with size 1646.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse  : 6.429551134903919\n"
     ]
    }
   ],
   "source": [
    "print('test rmse Jan 2023 using linear regresion model:')\n",
    "_ = evaluate_model(lr_model,encoded_df_test2,label_col,metric='rmse')\n",
    "# print('test rmse Jan 2023 using random forest regresion model:')\n",
    "# _ = evaluate_model(rf_model,encoded_df_test2,label_col,metric='rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test rmse Jan 2022 using linear regresion model:\n",
    "rmse  : 7.664661368978054\n",
    "- test rmse Jan 2022 using random forest regresion model:\n",
    "rmse  : 7.\n",
    "- test rmse Jan 2023 using linear regresion model:\n",
    "rmse  : 6.429551134910861\n",
    "- test rmse Jan 2023 using random forest regresion model:\n",
    "rmse  : 6.67\n",
    "##### By looking at the test results we can say linear regression is performing better in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_session = False\n",
    "if stop_session:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_processed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encoded_df.rdd.getNumPartitions())\n",
    "# encoded_df = encoded_df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.get(\"spark.storage.memoryFraction\")\n",
    "# from pyspark import StorageLevel\n",
    "# encoded_df = encoded_df.rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "# encoded_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"RDD Storage Level:\", encoded_df.getStorageLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols = ['trip_distance','VendorID_onehot','pu_hour_onehot','pu_weekday_onehot','PU_DO_onehot']\n",
    "# label_col = 'duration'\n",
    "# assembler = VectorAssembler(inputCols = feature_cols, outputCol = 'features')\n",
    "# regressor = RandomForestRegressor(featuresCol = 'features', labelCol= 'duration' )\n",
    "# pipeline = Pipeline(stages = [assembler,regressor])\n",
    "# model = pipeline.fit(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
