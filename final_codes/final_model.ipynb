{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, GBTRegressor\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import  StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import col, concat, lit, hour, dayofweek, round\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/abhishek-wsl/codes/MLops_project/final_codes/mlruns/1', creation_time=1693228766019, experiment_id='1', last_update_time=1693228766019, lifecycle_stage='active', name='modelling_duration_exp', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(experiment_name=\"modelling_duration_exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RegressorWithProcessing(mlflow.pyfunc.PythonModel):\n",
    "#     def __init__(self, params, model=None) -> None:\n",
    "#         from pyspark.sql import SparkSession\n",
    "#         self.params = params\n",
    "#         self.lr_model = model\n",
    "#         # self.indexer_final = indexer_final\n",
    "#         # self.encoder_final = encoder_final\n",
    "#         # self.spark = SparkSession.builder.appName('prod_model').getOrCreate()\n",
    "    \n",
    "#     def read_process_df(self, data_path):\n",
    "#         from pyspark.sql.functions import col, concat, lit, hour, dayofweek, round\n",
    "#         from pyspark.sql import SparkSession\n",
    "#         spark = SparkSession.builder.appName('prod_model').config(\"spark.driver.memory\", \"2g\").config(\"spark.executor.memory\", \"6g\")\\\n",
    "#     .config('spark.executor.cores',2).config('spark.default.parallelism',4).config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:2.2.0\").getOrCreate()\n",
    "\n",
    "#         df = spark.read.format('parquet').load(data_path)\n",
    "#         df = df.select('VendorID','lpep_pickup_datetime','lpep_dropoff_datetime','PULocationID','DOLocationID','trip_distance')\n",
    "#         df = df.withColumn('duration',\\\n",
    "#             round((col('lpep_dropoff_datetime')-col('lpep_pickup_datetime'))\\\n",
    "#             .cast(\"long\")/60,2))\n",
    "#         df = df.filter(col('duration')>=0.05).filter(col('duration')<=82)\n",
    "#         df = df.withColumn('PU_DO',concat(col('PULocationID'),lit('_'),col('DOLocationID')))\n",
    "#         df = df.withColumn('pu_hour',hour(col('lpep_pickup_datetime')))\n",
    "#         df = df.withColumn('pu_weekday',dayofweek(col('lpep_pickup_datetime')))\n",
    "        \n",
    "#         df = df.select('VendorID','pu_hour','pu_weekday','PU_DO', 'trip_distance','duration')\n",
    "#         # y = df.select('')\n",
    "\n",
    "#         print(df.count(), len(df.columns))\n",
    "#         return df\n",
    "\n",
    "    \n",
    "#     def prepare_data(self, data_path, categorical_cols, indexer_final=None, encoder_final=None, is_train=True):\n",
    "#         from pyspark.ml.feature import  StringIndexer, OneHotEncoder, VectorAssembler\n",
    "#         from pyspark.ml import Pipeline\n",
    "#         df_processed = self.read_process_df(data_path)\n",
    "#         indexers = [StringIndexer(inputCol=col,outputCol=col+'_index').fit(df_processed) for col in categorical_cols]\n",
    "#         if not is_train:\n",
    "#             df_processed = df_processed.dropna(subset='duration')\n",
    "#             [indexer.setHandleInvalid(\"keep\") for indexer in indexers]\n",
    "#         indexer_pipeline = Pipeline(stages=indexers)\n",
    "#         if indexer_final==None:\n",
    "#             indexer_final = indexer_pipeline.fit(df_processed)\n",
    "        \n",
    "#         indexed_df = indexer_final.transform(df_processed)\n",
    "\n",
    "#         encoder = [OneHotEncoder(inputCol=col+'_index',outputCol=col+'_onehot') for col in categorical_cols]\n",
    "#         encoder_pipeline = Pipeline(stages = encoder)\n",
    "#         if encoder_final==None:\n",
    "#             encoder_final = encoder_pipeline.fit(indexed_df)\n",
    "#         encoded_df = encoder_final.transform(indexed_df)\n",
    "        \n",
    "#         return encoded_df, indexer_final, encoder_final\n",
    "    \n",
    "#     def fit(self, data_path,feature_cols = ['trip_distance','VendorID_onehot','pu_hour_onehot','pu_weekday_onehot','PU_DO_onehot'],label_col='duration', categorical_cols = ['VendorID','pu_hour','pu_weekday','PU_DO'], regressor = LinearRegression,**kwargs):\n",
    "#         from pyspark.ml.feature import VectorAssembler\n",
    "#         from pyspark.ml import Pipeline\n",
    "#         encoded_df, indexer_final, encoder_final = self.prepare_data(data_path,categorical_cols)\n",
    "#         assembler = VectorAssembler(inputCols = feature_cols, outputCol = 'features')\n",
    "#         regressor = regressor(featuresCol = 'features', labelCol= label_col,**self.params,**kwargs )\n",
    "#         pipeline = Pipeline(stages = [assembler,regressor])\n",
    "#         model = pipeline.fit(encoded_df)\n",
    "#         self.model = model\n",
    "#         return model, indexer_final, encoder_final\n",
    "\n",
    "#     def predict(self, data_path, indexer_final=None, encoder_final=None, categorical_cols = ['VendorID','pu_hour','pu_weekday','PU_DO']):\n",
    "#         encoded_df, _, _ = self.prepare_data(data_path,categorical_cols, indexer_final, encoder_final,is_train=False)\n",
    "#         self.predictions = self.model.transform(encoded_df)\n",
    "#         return self.predictions.select('prediction')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/10 18:34:17 WARN Utils: Your hostname, Bhaiyu resolves to a loopback address: 127.0.1.1; using 172.17.120.207 instead (on interface eth0)\n",
      "23/09/10 18:34:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/abhishek-wsl/miniconda3/envs/mlops/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/abhishek-wsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/abhishek-wsl/.ivy2/jars\n",
      "org.mlflow#mlflow-spark added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1f19fdd7-8efd-47f9-90fd-7225100ef14d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mlflow#mlflow-spark;2.2.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      ":: resolution report :: resolve 145ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.mlflow#mlflow-spark;2.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1f19fdd7-8efd-47f9-90fd-7225100ef14d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "23/09/10 18:34:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('prod_model').config(\"spark.driver.memory\", \"2g\").config(\"spark.executor.memory\", \"6g\")\\\n",
    "    .config('spark.executor.cores', 2).config('spark.default.parallelism', 4).config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:2.2.0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressorWithProcessing(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, params, model=None) -> None:\n",
    "        self.params = params\n",
    "        self.lr_model = model\n",
    "        self.indexer_final = None\n",
    "        self.encoder_final = None\n",
    "\n",
    "    def read_process_df(self, df):\n",
    "        # from pyspark.sql.functions import col, concat, lit, hour, dayofweek, round\n",
    "        \n",
    "\n",
    "        #df = spark.read.format('parquet').load(data_path)\n",
    "        df = df.select('VendorID','lpep_pickup_datetime','lpep_dropoff_datetime','PULocationID','DOLocationID','trip_distance')\n",
    "        df = df.withColumn('duration',\\\n",
    "            round((col('lpep_dropoff_datetime')-col('lpep_pickup_datetime'))\\\n",
    "            .cast(\"long\")/60,2))\n",
    "        df = df.filter(col('duration')>=0.05).filter(col('duration')<=82)\n",
    "        df = df.withColumn('PU_DO',concat(col('PULocationID'),lit('_'),col('DOLocationID')))\n",
    "        df = df.withColumn('pu_hour',hour(col('lpep_pickup_datetime')))\n",
    "        df = df.withColumn('pu_weekday',dayofweek(col('lpep_pickup_datetime')))\n",
    "\n",
    "        df = df.select('VendorID','pu_hour','pu_weekday','PU_DO', 'trip_distance','duration')\n",
    "        print(df.count(), len(df.columns))\n",
    "        return df\n",
    "\n",
    "    def prepare_data(self, data_path, categorical_cols,is_train=True):\n",
    "        # from pyspark.ml.feature import  StringIndexer, OneHotEncoder, VectorAssembler\n",
    "        # from pyspark.ml import Pipeline\n",
    "        df_processed = self.read_process_df(data_path)\n",
    "        indexers = [StringIndexer(inputCol=col, outputCol=col+'_index').fit(df_processed) for col in categorical_cols]\n",
    "        if not is_train:\n",
    "            df_processed = df_processed.dropna(subset='duration')\n",
    "            [indexer.setHandleInvalid(\"keep\") for indexer in indexers]\n",
    "        indexer_pipeline = Pipeline(stages=indexers)\n",
    "        if self.indexer_final is None:\n",
    "            self.indexer_final = indexer_pipeline.fit(df_processed)\n",
    "\n",
    "        indexed_df = self.indexer_final.transform(df_processed)\n",
    "\n",
    "        encoder = [OneHotEncoder(inputCol=col+'_index', outputCol=col+'_onehot') for col in categorical_cols]\n",
    "        encoder_pipeline = Pipeline(stages=encoder)\n",
    "        if self.encoder_final is None:\n",
    "            self.encoder_final = encoder_pipeline.fit(indexed_df)\n",
    "\n",
    "        encoded_df = self.encoder_final.transform(indexed_df)\n",
    "\n",
    "        return encoded_df\n",
    "\n",
    "    def fit(self, data_path, feature_cols=['trip_distance','VendorID_onehot','pu_hour_onehot','pu_weekday_onehot','PU_DO_onehot'], label_col='duration', categorical_cols=['VendorID','pu_hour','pu_weekday','PU_DO'], regressor=LinearRegression, **kwargs):\n",
    "        # from pyspark.ml.feature import VectorAssembler\n",
    "        # from pyspark.ml import Pipeline\n",
    "        encoded_df = self.prepare_data(data_path, categorical_cols)\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "        regressor = regressor(featuresCol='features', labelCol=label_col, **self.params, **kwargs)\n",
    "        pipeline = Pipeline(stages=[assembler, regressor])\n",
    "        model = pipeline.fit(encoded_df)\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def predict(self, data_path, categorical_cols=['VendorID','pu_hour','pu_weekday','PU_DO']):\n",
    "        encoded_df = self.prepare_data(data_path, categorical_cols, is_train=False)\n",
    "        self.predictions = self.model.transform(encoded_df)\n",
    "        return self.predictions.select('prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'elasticNetParam':\t0.07666740311526124,\n",
    "            'fitIntercept':\tTrue,\n",
    "            'regParam':\t0.1452221778539822}\n",
    "\n",
    "model_obj = RegressorWithProcessing(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elasticNetParam': 0.07666740311526124,\n",
       " 'fitIntercept': True,\n",
       " 'regParam': 0.1452221778539822}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333262 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('parquet').load('/home/abhishek-wsl/codes/MLops_project/data/*.parquet')\n",
    "_ = model_obj.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333262 6\n"
     ]
    }
   ],
   "source": [
    "predictions = model_obj.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[prediction: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1 = model.spark.read.format('parquet').load('/home/abhishek-wsl/codes/MLops_project/data/test_data/green_tripdata_2023-01.parquet')\n",
    "# df_1 = df_1.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlflow.models.signature import infer_signature\n",
    "\n",
    "# signature = infer_signature(df_1, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.RLock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#using user defined python class containing pyspark preprocessing and ml model from mlflow.pyfunc.PythonMode\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlr_model_test1\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m run:\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     mlflow\u001b[39m.\u001b[39;49mpyfunc\u001b[39m.\u001b[39;49mlog_model(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mlr_preprocessed_model\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         python_model\u001b[39m=\u001b[39;49mmodel_obj\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m# artifacts=artifacts,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m# conda_env= conda_env,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m# signature = signature,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m# input_example=df_1.toPandas()\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# mlflow.spark.log_model(indexer_final,artifact_path='indexer')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# mlflow.spark.log_model(encoder_final,artifact_path='encoder')\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py:1995\u001b[0m, in \u001b[0;36mlog_model\u001b[0;34m(artifact_path, loader_module, data_path, code_path, conda_env, python_model, artifacts, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, metadata)\u001b[0m\n\u001b[1;32m   1837\u001b[0m \u001b[39m@format_docstring\u001b[39m(LOG_MODEL_PARAM_DOCS\u001b[39m.\u001b[39mformat(package_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mscikit-learn\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1838\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_model\u001b[39m(\n\u001b[1;32m   1839\u001b[0m     artifact_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1852\u001b[0m     metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1853\u001b[0m ):\n\u001b[1;32m   1854\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1855\u001b[0m \u001b[39m    Log a Pyfunc model with custom inference logic and optional data dependencies as an MLflow\u001b[39;00m\n\u001b[1;32m   1856\u001b[0m \u001b[39m    artifact for the current run.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[39m             metadata of the logged model.\u001b[39;00m\n\u001b[1;32m   1994\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1995\u001b[0m     \u001b[39mreturn\u001b[39;00m Model\u001b[39m.\u001b[39;49mlog(\n\u001b[1;32m   1996\u001b[0m         artifact_path\u001b[39m=\u001b[39;49martifact_path,\n\u001b[1;32m   1997\u001b[0m         flavor\u001b[39m=\u001b[39;49mmlflow\u001b[39m.\u001b[39;49mpyfunc,\n\u001b[1;32m   1998\u001b[0m         loader_module\u001b[39m=\u001b[39;49mloader_module,\n\u001b[1;32m   1999\u001b[0m         data_path\u001b[39m=\u001b[39;49mdata_path,\n\u001b[1;32m   2000\u001b[0m         code_path\u001b[39m=\u001b[39;49mcode_path,\n\u001b[1;32m   2001\u001b[0m         python_model\u001b[39m=\u001b[39;49mpython_model,\n\u001b[1;32m   2002\u001b[0m         artifacts\u001b[39m=\u001b[39;49martifacts,\n\u001b[1;32m   2003\u001b[0m         conda_env\u001b[39m=\u001b[39;49mconda_env,\n\u001b[1;32m   2004\u001b[0m         registered_model_name\u001b[39m=\u001b[39;49mregistered_model_name,\n\u001b[1;32m   2005\u001b[0m         signature\u001b[39m=\u001b[39;49msignature,\n\u001b[1;32m   2006\u001b[0m         input_example\u001b[39m=\u001b[39;49minput_example,\n\u001b[1;32m   2007\u001b[0m         await_registration_for\u001b[39m=\u001b[39;49mawait_registration_for,\n\u001b[1;32m   2008\u001b[0m         pip_requirements\u001b[39m=\u001b[39;49mpip_requirements,\n\u001b[1;32m   2009\u001b[0m         extra_pip_requirements\u001b[39m=\u001b[39;49mextra_pip_requirements,\n\u001b[1;32m   2010\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m   2011\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/mlflow/models/model.py:579\u001b[0m, in \u001b[0;36mModel.log\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m     (tracking_uri \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatabricks\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m get_uri_scheme(tracking_uri) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatabricks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    575\u001b[0m     \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msignature\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    576\u001b[0m     \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39minput_example\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    577\u001b[0m ):\n\u001b[1;32m    578\u001b[0m     _logger\u001b[39m.\u001b[39mwarning(_LOG_MODEL_MISSING_SIGNATURE_WARNING)\n\u001b[0;32m--> 579\u001b[0m flavor\u001b[39m.\u001b[39;49msave_model(path\u001b[39m=\u001b[39;49mlocal_path, mlflow_model\u001b[39m=\u001b[39;49mmlflow_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    580\u001b[0m mlflow\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mfluent\u001b[39m.\u001b[39mlog_artifacts(local_path, mlflow_model\u001b[39m.\u001b[39martifact_path)\n\u001b[1;32m    581\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py:1823\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(path, loader_module, data_path, code_path, conda_env, mlflow_model, python_model, artifacts, signature, input_example, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     \u001b[39mreturn\u001b[39;00m _save_model_with_loader_module_and_data_path(\n\u001b[1;32m   1813\u001b[0m         path\u001b[39m=\u001b[39mpath,\n\u001b[1;32m   1814\u001b[0m         loader_module\u001b[39m=\u001b[39mloader_module,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1820\u001b[0m         extra_pip_requirements\u001b[39m=\u001b[39mextra_pip_requirements,\n\u001b[1;32m   1821\u001b[0m     )\n\u001b[1;32m   1822\u001b[0m \u001b[39melif\u001b[39;00m second_argument_set_specified:\n\u001b[0;32m-> 1823\u001b[0m     \u001b[39mreturn\u001b[39;00m mlflow\u001b[39m.\u001b[39;49mpyfunc\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49m_save_model_with_class_artifacts_params(\n\u001b[1;32m   1824\u001b[0m         path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1825\u001b[0m         signature\u001b[39m=\u001b[39;49msignature,\n\u001b[1;32m   1826\u001b[0m         hints\u001b[39m=\u001b[39;49mhints,\n\u001b[1;32m   1827\u001b[0m         python_model\u001b[39m=\u001b[39;49mpython_model,\n\u001b[1;32m   1828\u001b[0m         artifacts\u001b[39m=\u001b[39;49martifacts,\n\u001b[1;32m   1829\u001b[0m         conda_env\u001b[39m=\u001b[39;49mconda_env,\n\u001b[1;32m   1830\u001b[0m         code_paths\u001b[39m=\u001b[39;49mcode_path,\n\u001b[1;32m   1831\u001b[0m         mlflow_model\u001b[39m=\u001b[39;49mmlflow_model,\n\u001b[1;32m   1832\u001b[0m         pip_requirements\u001b[39m=\u001b[39;49mpip_requirements,\n\u001b[1;32m   1833\u001b[0m         extra_pip_requirements\u001b[39m=\u001b[39;49mextra_pip_requirements,\n\u001b[1;32m   1834\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/mlflow/pyfunc/model.py:225\u001b[0m, in \u001b[0;36m_save_model_with_class_artifacts_params\u001b[0;34m(path, python_model, signature, hints, artifacts, conda_env, code_paths, mlflow_model, pip_requirements, extra_pip_requirements)\u001b[0m\n\u001b[1;32m    223\u001b[0m saved_python_model_subpath \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpython_model.pkl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, saved_python_model_subpath), \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m out:\n\u001b[0;32m--> 225\u001b[0m     cloudpickle\u001b[39m.\u001b[39;49mdump(python_model, out)\n\u001b[1;32m    226\u001b[0m custom_model_config_kwargs[CONFIG_KEY_PYTHON_MODEL] \u001b[39m=\u001b[39m saved_python_model_subpath\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m artifacts:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py:55\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, buffer_callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Serialize obj as bytes streamed into file\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[39m    protocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m    compatibility with older versions of Python.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     CloudPickler(\n\u001b[1;32m     56\u001b[0m         file, protocol\u001b[39m=\u001b[39;49mprotocol, buffer_callback\u001b[39m=\u001b[39;49mbuffer_callback\n\u001b[1;32m     57\u001b[0m     )\u001b[39m.\u001b[39;49mdump(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(\u001b[39mself\u001b[39m, obj):\n\u001b[1;32m    631\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m         \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[1;32m    633\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    634\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrecursion\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.RLock' object"
     ]
    }
   ],
   "source": [
    "#using user defined python class containing pyspark preprocessing and ml model from mlflow.pyfunc.PythonMode\n",
    "with mlflow.start_run(run_name='lr_model_test1') as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        'lr_preprocessed_model',\n",
    "        python_model=model_obj\n",
    "        # artifacts=artifacts,\n",
    "        # conda_env= conda_env,\n",
    "        # signature = signature,\n",
    "        # input_example=df_1.toPandas()\n",
    "    )\n",
    "    # mlflow.spark.log_model(indexer_final,artifact_path='indexer')\n",
    "    # mlflow.spark.log_model(encoder_final,artifact_path='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'socket' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdill\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m serialized_object \u001b[39m=\u001b[39m dill\u001b[39m.\u001b[39;49mdumps(model_obj)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSerialization successful.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# except Exception as e:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/abhishek-wsl/codes/MLops_project/final_codes/final_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#     print(f\"Serialization failed: {e}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:278\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, byref, fmode, recurse, **kwds)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39mPickle an object to a string.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mDefault values for keyword arguments can be set in :mod:`dill.settings`.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m file \u001b[39m=\u001b[39m StringIO()\n\u001b[0;32m--> 278\u001b[0m dump(obj, file, protocol, byref, fmode, recurse, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\u001b[39m#, strictio)\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:250\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol, byref, fmode, recurse, **kwds)\u001b[0m\n\u001b[1;32m    248\u001b[0m _kwds \u001b[39m=\u001b[39m kwds\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    249\u001b[0m _kwds\u001b[39m.\u001b[39mupdate(\u001b[39mdict\u001b[39m(byref\u001b[39m=\u001b[39mbyref, fmode\u001b[39m=\u001b[39mfmode, recurse\u001b[39m=\u001b[39mrecurse))\n\u001b[0;32m--> 250\u001b[0m Pickler(file, protocol, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwds)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    251\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:418\u001b[0m, in \u001b[0;36mPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(\u001b[39mself\u001b[39m, obj): \u001b[39m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     logger\u001b[39m.\u001b[39mtrace_setup(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 418\u001b[0m     StockPickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mstart_framing()\n\u001b[0;32m--> 487\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(obj)\n\u001b[1;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(STOP)\n\u001b[1;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mend_framing()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:412\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    410\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    411\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 412\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:412\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    410\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    411\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 412\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:1212\u001b[0m, in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[1;32m   1210\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1212\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[1;32m   1213\u001b[0m     logger\u001b[39m.\u001b[39mtrace(pickler, \u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1214\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 971\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:997\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    996\u001b[0m         save(k)\n\u001b[0;32m--> 997\u001b[0m         save(v)\n\u001b[1;32m    998\u001b[0m     write(SETITEMS)\n\u001b[1;32m    999\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "    \u001b[0;31m[... skipping similar frames: Pickler.save at line 412 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Pickler.save at line 412 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:1212\u001b[0m, in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[1;32m   1210\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1212\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[1;32m   1213\u001b[0m     logger\u001b[39m.\u001b[39mtrace(pickler, \u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1214\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 971\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:997\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    996\u001b[0m         save(k)\n\u001b[0;32m--> 997\u001b[0m         save(v)\n\u001b[1;32m    998\u001b[0m     write(SETITEMS)\n\u001b[1;32m    999\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:412\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    410\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    411\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 412\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:931\u001b[0m, in \u001b[0;36m_Pickler.save_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m LIST)\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 931\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_appends(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:955\u001b[0m, in \u001b[0;36m_Pickler._batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    953\u001b[0m     write(MARK)\n\u001b[1;32m    954\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tmp:\n\u001b[0;32m--> 955\u001b[0m         save(x)\n\u001b[1;32m    956\u001b[0m     write(APPENDS)\n\u001b[1;32m    957\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:412\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    410\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    411\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 412\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:412\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    410\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    411\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 412\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:1212\u001b[0m, in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[1;32m   1210\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1212\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[1;32m   1213\u001b[0m     logger\u001b[39m.\u001b[39mtrace(pickler, \u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1214\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 971\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:997\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    996\u001b[0m         save(k)\n\u001b[0;32m--> 997\u001b[0m         save(v)\n\u001b[1;32m    998\u001b[0m     write(SETITEMS)\n\u001b[1;32m    999\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "    \u001b[0;31m[... skipping similar frames: Pickler.save at line 412 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Pickler.save at line 412 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:1212\u001b[0m, in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[1;32m   1210\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1212\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[1;32m   1213\u001b[0m     logger\u001b[39m.\u001b[39mtrace(pickler, \u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1214\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 971\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:997\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    996\u001b[0m         save(k)\n\u001b[0;32m--> 997\u001b[0m         save(v)\n\u001b[1;32m    998\u001b[0m     write(SETITEMS)\n\u001b[1;32m    999\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "    \u001b[0;31m[... skipping similar frames: Pickler.save at line 412 (1 times), _Pickler.save at line 603 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Pickler.save at line 412 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:1212\u001b[0m, in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[1;32m   1210\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1212\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[1;32m   1213\u001b[0m     logger\u001b[39m.\u001b[39mtrace(pickler, \u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1214\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 971\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:997\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    996\u001b[0m         save(k)\n\u001b[0;32m--> 997\u001b[0m         save(v)\n\u001b[1;32m    998\u001b[0m     write(SETITEMS)\n\u001b[1;32m    999\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:412\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    410\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    411\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 412\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:710\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39m# More new special cases (that work with older protocols as\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39m# well): when __reduce__ returns a tuple with 4 or 5 items,\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[39m# the 4th and 5th item should be iterators that provide list\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[39m# items and dict items (as (key, value) tuples), or None.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[39mif\u001b[39;00m listitems \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_appends(listitems)\n\u001b[1;32m    712\u001b[0m \u001b[39mif\u001b[39;00m dictitems \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_setitems(dictitems)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:955\u001b[0m, in \u001b[0;36m_Pickler._batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    953\u001b[0m     write(MARK)\n\u001b[1;32m    954\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tmp:\n\u001b[0;32m--> 955\u001b[0m         save(x)\n\u001b[1;32m    956\u001b[0m     write(APPENDS)\n\u001b[1;32m    957\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:412\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    410\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    411\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 412\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:412\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    410\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    411\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 412\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:1212\u001b[0m, in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[1;32m   1210\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1212\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[1;32m   1213\u001b[0m     logger\u001b[39m.\u001b[39mtrace(pickler, \u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1214\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 971\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:997\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    996\u001b[0m         save(k)\n\u001b[0;32m--> 997\u001b[0m         save(v)\n\u001b[1;32m    998\u001b[0m     write(SETITEMS)\n\u001b[1;32m    999\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/site-packages/dill/_dill.py:412\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    410\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    411\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 412\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/pickle.py:578\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    576\u001b[0m reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce_ex__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     rv \u001b[39m=\u001b[39m reduce(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproto)\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.9/socket.py:273\u001b[0m, in \u001b[0;36msocket.__getstate__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getstate__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcannot pickle \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m object\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'socket' object"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "\n",
    "serialized_object = dill.dumps(model_obj)\n",
    "print(\"Serialization successful.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Serialization failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
