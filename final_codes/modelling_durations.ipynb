{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import  StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# from utils import read_process_df, prepare_data, evaluate_model, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.120.207:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>modelling</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f01e53e6250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('modelling').config(\"spark.driver.memory\", \"2g\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2g'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.driver.memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()\n",
    "#df.show(5)\n",
    "#df.select('VendorID').distinct().show()\n",
    "#df.select('duration').summary(\"count\", \"min\", \"1%\",\"25%\", \"50%\", \"75%\", \"95%\", \"98%\",  \"99%\",\"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- minimum duration is -54\n",
    "- max looks like heavily skewed\n",
    "- lets consider minimum as 0.05 min (1 percentile) and max as 82 mins (99 percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_process_df(path):\n",
    "    df = spark.read.format('parquet').load(path)\n",
    "    df = df.select('VendorID','lpep_pickup_datetime','lpep_dropoff_datetime','PULocationID','DOLocationID','trip_distance')\n",
    "    df = df.withColumn('duration',\\\n",
    "        round((col('lpep_dropoff_datetime')-col('lpep_pickup_datetime'))\\\n",
    "        .cast(\"long\")/60,2))\n",
    "    df = df.filter(col('duration')>=0.05).filter(col('duration')<=82)\n",
    "    df = df.withColumn('PU_DO',concat(col('PULocationID'),lit('_'),col('DOLocationID')))\n",
    "    df = df.withColumn('pu_hour',hour(col('lpep_pickup_datetime')))\n",
    "    df = df.withColumn('pu_weekday',dayofweek(col('lpep_pickup_datetime')))\n",
    "    \n",
    "    df = df.select('VendorID','pu_hour','pu_weekday','PU_DO', 'trip_distance','duration')\n",
    "    # y = df.select('')\n",
    "\n",
    "    print(df.count(), len(df.columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = sdf.toPandas() #.describe()\n",
    "# sdf = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_processed,categorical_cols,indexer_final=None,encoder_final=None, is_test=True):\n",
    "    indexers = [StringIndexer(inputCol=col,outputCol=col+'_index').fit(df_processed) for col in categorical_cols]\n",
    "    if is_test:\n",
    "        df_processed = df_processed.dropna(subset='duration')\n",
    "        [indexer.setHandleInvalid(\"keep\") for indexer in indexers]\n",
    "    indexer_pipeline = Pipeline(stages=indexers)\n",
    "    if indexer_final==None:\n",
    "        indexer_final = indexer_pipeline.fit(df_processed)\n",
    "    \n",
    "    indexed_df = indexer_final.transform(df_processed)\n",
    "\n",
    "    encoder = [OneHotEncoder(inputCol=col+'_index',outputCol=col+'_onehot') for col in categorical_cols]\n",
    "    encoder_pipeline = Pipeline(stages = encoder)\n",
    "    if encoder_final==None:\n",
    "        encoder_final = encoder_pipeline.fit(indexed_df)\n",
    "    encoded_df = encoder_final.transform(indexed_df)\n",
    "    \n",
    "    return encoded_df, indexer_final, encoder_final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoded_df,feature_cols,label_col,regressor = LinearRegression):\n",
    "    assembler = VectorAssembler(inputCols = feature_cols, outputCol = 'features')\n",
    "    regressor = regressor(featuresCol = 'features', labelCol= label_col )\n",
    "    pipeline = Pipeline(stages = [assembler,regressor])\n",
    "    model = pipeline.fit(encoded_df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,encoded_df,label_col,metric='rmse'):\n",
    "    predictions = model.transform(encoded_df)\n",
    "    evaluator = RegressionEvaluator(labelCol=label_col,predictionCol='prediction',metricName=metric)\n",
    "    out = evaluator.evaluate(predictions)\n",
    "    print(f\"{metric}  : {out}\")\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333262 6\n",
      "61753 6\n",
      "67613 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train_processed = read_process_df('/home/abhishek-wsl/codes/MLops_project/data/*.parquet')\n",
    "df_test_processed1 = read_process_df('/home/abhishek-wsl/codes/MLops_project/data/test_data/green_tripdata_2022-01.parquet')\n",
    "df_test_processed2 = read_process_df('/home/abhishek-wsl/codes/MLops_project/data/test_data/green_tripdata_2023-01.parquet')\n",
    "\n",
    "categorical_cols = ['VendorID','pu_hour','pu_weekday','PU_DO']\n",
    "encoded_df_train, indexer_final, encoder_final = prepare_data(df_train_processed,categorical_cols)\n",
    "encoded_df_test1, _, _ = prepare_data(df_test_processed1,categorical_cols,indexer_final,encoder_final,is_test=True)\n",
    "encoded_df_test2, _, _ = prepare_data(df_test_processed2,categorical_cols,indexer_final,encoder_final,is_test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/27 00:02:40 WARN DAGScheduler: Broadcasting large task binary with size 1545.3 KiB\n",
      "23/08/27 00:02:42 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/08/27 00:02:42 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:43 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:44 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:44 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:44 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:44 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:44 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:44 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:44 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:44 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:44 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:45 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:45 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:45 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:45 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:45 WARN DAGScheduler: Broadcasting large task binary with size 1546.0 KiB\n",
      "23/08/27 00:02:45 WARN DAGScheduler: Broadcasting large task binary with size 1645.7 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_cols = ['trip_distance','VendorID_onehot','pu_hour_onehot','pu_weekday_onehot','PU_DO_onehot']\n",
    "label_col = 'duration'\n",
    "lr_model = train_model(encoded_df_train,feature_cols,label_col)\n",
    "#rf_model = train_model(encoded_df_train,feature_cols,label_col,regressor=RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models=False\n",
    "if save_models:\n",
    "    lr_model.save( \"/home/abhishek-wsl/codes/MLops_project/trained_models/lr_PipelineModel\")\n",
    "    rf_model.save(\"/home/abhishek-wsl/codes/MLops_project/trained_models/rf_PipelineModel\")\n",
    "    indexer_final.save(\"/home/abhishek-wsl/codes/MLops_project/trained_models/stringindexer_PipelineModel\")\n",
    "    encoder_final.save(\"/home/abhishek-wsl/codes/MLops_project/trained_models/encoderindexer_PipelineModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training rmse using linear regresion model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/27 00:03:00 WARN DAGScheduler: Broadcasting large task binary with size 1646.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse  : 6.55947373309185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('training rmse using linear regresion model:')\n",
    "_ = evaluate_model(lr_model,encoded_df_train,label_col,metric='rmse')\n",
    "# print('training rmse using random forest regresion model:')\n",
    "# _ = evaluate_model(rf_model,encoded_df_train,label_col,metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rmse Jan 2022 using linear regresion model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/27 00:03:06 WARN DAGScheduler: Broadcasting large task binary with size 1646.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse  : 7.664661368974521\n"
     ]
    }
   ],
   "source": [
    "print('test rmse Jan 2022 using linear regresion model:')\n",
    "_ = evaluate_model(lr_model,encoded_df_test1,label_col,metric='rmse')\n",
    "# print('test rmse Jan 2022 using random forest regresion model:')\n",
    "# _ = evaluate_model(rf_model,encoded_df_test1,label_col,metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rmse Jan 2023 using linear regresion model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/27 00:03:22 WARN DAGScheduler: Broadcasting large task binary with size 1646.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse  : 6.429551134903919\n"
     ]
    }
   ],
   "source": [
    "print('test rmse Jan 2023 using linear regresion model:')\n",
    "_ = evaluate_model(lr_model,encoded_df_test2,label_col,metric='rmse')\n",
    "# print('test rmse Jan 2023 using random forest regresion model:')\n",
    "# _ = evaluate_model(rf_model,encoded_df_test2,label_col,metric='rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test rmse Jan 2022 using linear regresion model:\n",
    "rmse  : 7.664661368978054\n",
    "- test rmse Jan 2022 using random forest regresion model:\n",
    "rmse  : 7.\n",
    "- test rmse Jan 2023 using linear regresion model:\n",
    "rmse  : 6.429551134910861\n",
    "- test rmse Jan 2023 using random forest regresion model:\n",
    "rmse  : 6.67\n",
    "##### By looking at the test results we can say linear regression is performing better in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_session = False\n",
    "if stop_session:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_processed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encoded_df.rdd.getNumPartitions())\n",
    "# encoded_df = encoded_df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.get(\"spark.storage.memoryFraction\")\n",
    "# from pyspark import StorageLevel\n",
    "# encoded_df = encoded_df.rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "# encoded_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"RDD Storage Level:\", encoded_df.getStorageLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols = ['trip_distance','VendorID_onehot','pu_hour_onehot','pu_weekday_onehot','PU_DO_onehot']\n",
    "# label_col = 'duration'\n",
    "# assembler = VectorAssembler(inputCols = feature_cols, outputCol = 'features')\n",
    "# regressor = RandomForestRegressor(featuresCol = 'features', labelCol= 'duration' )\n",
    "# pipeline = Pipeline(stages = [assembler,regressor])\n",
    "# model = pipeline.fit(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
